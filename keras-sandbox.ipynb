{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Deep Learning\n",
    "First try to use Keras\n",
    "Based on\n",
    "\n",
    "https://nlpforhackers.io/keras-intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding, LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 500\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encodeY(Y):\n",
    "    '''create one-hot (dummies) for output, see also https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "    encode class values as integers\n",
    "    '''\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    return dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     11124\n",
      "False     1435\n",
      "Name: v9_major, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pickle.load(open('newsdataset/news_dataset_annotated.pkl', mode ='rb'))\n",
    "# remove missing values\n",
    "print(df['v9_major'].map(str.isdigit).value_counts())\n",
    "df = df[df['v9_major'].map(str.isdigit)]\n",
    "# remove niche categories\n",
    "df = df[df['v9_major'].map(df.v9_major.value_counts()>150)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], encodeY(df['v9_major'].map(int)), test_size = 0.2)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=.9)\n",
    "X_train_onehot = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dit zou werken als y_train nog steeds een pandas-series was\n",
    "# numberoflabels = len(y_train.unique())\n",
    "# maar dat is het dankzij onze encodeY-functie niet meer.\n",
    "# vandaar even een hele lelijke hack:\n",
    "numberoflabels = len(set((str(e) for e in y_train.tolist())))\n",
    "numberoflabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense (units = 500, activation = 'relu', input_dim = len(vectorizer.get_feature_names())))\n",
    "#model.add(Dense(units = 400, activation = 'sigmoid'))\n",
    "#model.add(Dense(units = 300, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 200, activation = 'sigmoid'))\n",
    "# model.add(Dense(units = 100, activation = 'sigmoid'))\n",
    "model.add(Dense(units = numberoflabels, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 500)               11062000  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 19)                3819      \n",
      "=================================================================\n",
      "Total params: 11,166,019\n",
      "Trainable params: 11,166,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7712 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "7712/7712 [==============================] - 34s 4ms/step - loss: 1.9151 - acc: 0.4440 - val_loss: 1.2842 - val_acc: 0.6300\n",
      "Epoch 2/3\n",
      "7712/7712 [==============================] - 32s 4ms/step - loss: 0.6377 - acc: 0.8301 - val_loss: 1.2629 - val_acc: 0.6180\n",
      "Epoch 3/3\n",
      "7712/7712 [==============================] - 34s 4ms/step - loss: 0.1773 - acc: 0.9620 - val_loss: 1.2799 - val_acc: 0.6420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f337744cc18>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_onehot[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n",
    "          epochs=3, verbose=1,\n",
    "          validation_data=(X_train_onehot[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053/2053 [==============================] - 1s 530us/step\n",
      "Accuracy: 0.601558694622311\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(vectorizer.transform(X_test), y_test, verbose = True)\n",
    "print(\"Accuracy: {}\".format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "tokenize = vectorizer.build_tokenizer()\n",
    "preprocess = vectorizer.build_preprocessor()\n",
    " \n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    "\n",
    "X_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_train] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SEQ_LENGHT= 3197\n",
      "[22123 22123 22123 ... 16975  7401 16977]\n"
     ]
    }
   ],
   "source": [
    "# Compute the max lenght of a text\n",
    "MAX_SEQ_LENGHT = len(max(X_train_sequences, key=len))\n",
    "print(\"MAX_SEQ_LENGHT=\", MAX_SEQ_LENGHT)\n",
    " \n",
    "N_FEATURES = len(vectorizer.get_feature_names())\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n",
    "print(X_train_sequences[0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3197, 64)          1415936   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3193, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 638, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 40832)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2613312   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 19)                1235      \n",
      "=================================================================\n",
      "Total params: 4,051,027\n",
      "Trainable params: 4,051,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
    "                    64,  # Embedding size\n",
    "                    input_length=MAX_SEQ_LENGHT))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=numberoflabels, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7712 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "7712/7712 [==============================] - 87s 11ms/step - loss: 2.6948 - acc: 0.1154 - val_loss: 2.5978 - val_acc: 0.1560\n",
      "Epoch 2/3\n",
      "7712/7712 [==============================] - 83s 11ms/step - loss: 2.2819 - acc: 0.2313 - val_loss: 2.1789 - val_acc: 0.2940\n",
      "Epoch 3/3\n",
      "4608/7712 [================>.............] - ETA: 33s - loss: 1.5714 - acc: 0.4735"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_sequences[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n",
    "          epochs=3, verbose=True,\n",
    "          validation_data=(X_train_sequences[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_test]\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1]) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
    "                    64,  # Embedding size\n",
    "                    input_length=MAX_SEQ_LENGHT))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(units=numberoflabels, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_sequences[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n",
    "          epochs=2, batch_size=128, verbose=1, \n",
    "          validation_data=(X_train_sequences[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_sequences, y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained embeddings\n",
    "\n",
    "bla\n",
    "bla\n",
    "\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(open('aem/w2v_320d_AEM_corpus_2000-01-01_2017-12-31', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657264089"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.wv.word_vec('morgen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://adventuresinmachinelearning.com/gensim-word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(embeddings.wv.vocab), embeddings.vector_size))\n",
    "for i in range(len(embeddings.wv.vocab)):\n",
    "    embedding_vector = embeddings.wv[embeddings.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1055100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numberoflabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2be04a7977e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumberoflabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numberoflabels' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "#model.add(Embedding(len(vectorizer.get_feature_names()) + 1,\n",
    "#                    64,  # Embedding size\n",
    "#                    input_length=MAX_SEQ_LENGHT))\n",
    "\n",
    "#model.add(embeddings.wv.get_keras_embedding())\n",
    "\n",
    "model.add(Embedding(input_dim = 1055100, \n",
    "                    weights=[embedding_matrix], \n",
    "                    output_dim=320, trainable=False,\n",
    "                    input_shape= ()))\n",
    "\n",
    "#model.add(Conv1D(320, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(5))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "model.add(Dense(units = 200, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=numberoflabels, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embeddings.wv.get_keras_embedding())\n",
    "# model.add(Dense (units = 500, activation = 'relu', input_dim = len(vectorizer.get_feature_names())))\n",
    "#model.add(Dense(units = 400, activation = 'sigmoid'))\n",
    "#model.add(Dense(units = 300, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 200, activation = 'sigmoid'))\n",
    "# model.add(Dense(units = 100, activation = 'sigmoid'))\n",
    "model.add(Dense(units = numberoflabels, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 320)         337632000 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 200)         64200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 19)          3819      \n",
      "=================================================================\n",
      "Total params: 337,700,019\n",
      "Trainable params: 68,019\n",
      "Non-trainable params: 337,632,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have 3 dimensions, but got array with shape (7712, 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-feec1a6f0004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train_onehot[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           validation_data=(X_train_onehot[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 3 dimensions, but got array with shape (7712, 19)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_onehot[:-VALIDATION_SIZE], y_train[:-VALIDATION_SIZE], \n",
    "          epochs=3, verbose=1,\n",
    "          validation_data=(X_train_onehot[-VALIDATION_SIZE:], y_train[-VALIDATION_SIZE:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(vectorizer.transform(X_test), y_test, verbose = True)\n",
    "print(\"Accuracy: {}\".format(scores[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
